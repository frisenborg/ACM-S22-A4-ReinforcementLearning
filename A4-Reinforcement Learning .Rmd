---
title: "A4-Reinforcement Learning"
author: "MV + gang"
date: "4/27/2022"
output: html_document
---

## Intructions

Part 1 - You have to design a study (aka plan the number of trials) assuming that people are using a reinforcement learning process to pick between 2 stimuli. In this study you expect there to be 2 conditions and the 1 participant playing the game will vary its learning rate between conditions. The difference in learning rate is .2: 
Condition 1: has x - .1 
Condition 2: x + .1
with x = 0.7. 
The temperature is the same: 0.5.


Identify a feasible number of trials and motivate it.
[optional]: what happens if x is not = +.7 (tip: test a range of different x)?
[optional]: what happens if temperature is not 0.5, but 5?


Part 2 - Given the large number of trials required, could you imagine producing an iterated design? E.g. a phone app where you can do a smaller number of trials (e.g. 10-20 or even 100, up to you!) in separate sessions, each time a posterior is generated and it is used as prior in the next time.
Assuming no variance over time (ah!) can you figure out a good trade off between how many trials per session and number of sessions?
[optional]: what are the differences in just re-running the model on the cumulative dataset (increased at every session) vs passing the posterior? Differences in terms of computational time, estimates, but also practical implication for running your study.
[optional]: what happens if learning rate changes a bit across sessions? Include a variation between sessions according to a normal distribution with a mean of 0 and a sd of 0.02. Re-assess the number of trials/sessions used.

Link for the written part of the assignment: https://docs.google.com/document/d/1FSt2rj1M7e8U3pK3zZD7gYwxpXsnlpQBRS-S8NwNwQU/edit 

# Setup 
```{r setup}
# packages 
library("pacman")
p_load(tidyverse, here, posterior, cmdstanr, rstan, brms)

# use all computer cores 
options(mc.cores = parallel::detectCores())
rstan_options(auto.write = TRUE)
```


```{r functions}

softmax <- function(x, tau){
  outcome = 1 / (1 + exp(-tau * x))
  return(outcome)
}

value_update <- function(value, alpha, choice, feedback){
  
  pe <- feedback - value
  v1 <- value[1] + alpha * (1 - choice) * (feedback - value[1])
  v2 <- value[2] + alpha * (choice) * (feedback - value[2])
  
  updated_value <- c(v1, v2)
  return(updated_value)
}


generate_fate <- function(trials, p){
  fate <- rbinom(trials, 1, p)
  fate <- fate + 1
  return(fate)
}


agent <- function(fate, alpha, trials, temperature = 0.5, init_value = 0.5){
  
  data <- tibble(choice = rep(NA, trials),
                 fate = rep(NA, trials),
                 value1 = rep(NA, trials),
                 value2 = rep(NA, trials),
                 feedback = rep(NA, trials))
  
  value <- c(init_value, init_value)
  
  for (trial in 1:trials){
    
    choice <- rbinom(1, 1, softmax(value[2] - value[1], temperature))
    choice <- choice + 1
    feedback <- ifelse(fate[trial] == choice, 1, 0)
    value <- value_update(value-1, alpha, choice, feedback)
    
    data$choice[trial] <- choice
    data$fate[trial] <- fate[trial]
    data$value1[trial] <- value[1]
    data$value2[trial] <- value[2]
    data$feedback[trial] <- feedback
  }
  
  return(data)

}


```


# Stan
```{r stan}
# variables 
alpha <- 0.9
temperature <- 0.5
init_value <- 0.5
trials <- 100
p <- 0.5

# 
fate <- generate_fate(trials, p)
df <- agent(fate, alpha, trials, temperature, init_value)

stan_model <- cmdstan_model("stan_models/reinforcementlearning.stan")

stan_data <- list(
  trials = trials, 
  feedback = df$feedback,
  choice = df$choice,
  condition = df$fate,
  initValue = init_value
)

reinforcement_learning <- stan_model$sample(
  data = stan_data,
  seed = 345,
  chains = 1, 
  parallel_chains = 1, 
  iter_warmup = 1000, 
  iter_sampling = 2000,
  refresh = 100
)


```


